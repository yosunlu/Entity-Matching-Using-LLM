{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ed23b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "from pdb import set_trace\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "731130fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [\n",
    "    \n",
    "    'wdcproducts-80cc-seen-sampled-250-gs-2_domain-simple-free',\n",
    "    'wdcproducts-80cc-seen-sampled-250-gs-2_domain-complex-free',\n",
    "    'wdcproducts-80cc-seen-sampled-250-gs-2_domain-simple-force',\n",
    "    'wdcproducts-80cc-seen-sampled-250-gs-2_domain-complex-force',\n",
    "    'wdcproducts-80cc-seen-sampled-250-gs-2_general-simple-free',\n",
    "    'wdcproducts-80cc-seen-sampled-250-gs-2_general-complex-free',\n",
    "    'wdcproducts-80cc-seen-sampled-250-gs-2_general-simple-force',\n",
    "    'wdcproducts-80cc-seen-sampled-250-gs-2_general-complex-force',\n",
    "\n",
    "    'wdcproducts-80cc-seen-sampled-250-gs-2_domain-simple-force-rules-heterogeneity',\n",
    "    'wdcproducts-80cc-seen-sampled-250-gs-2_domain-simple-force-rules-gen-handpicked',\n",
    "    \n",
    "    'wdcproducts-80cc-seen-sampled-250-gs-2_relatedwork-simple',\n",
    "    'wdcproducts-80cc-seen-sampled-250-gs-2_relatedwork-complex',\n",
    "    \n",
    "    'abt-buy-sampled-gs_domain-simple-free',\n",
    "    'abt-buy-sampled-gs_domain-complex-free',\n",
    "    'abt-buy-sampled-gs_domain-simple-force',\n",
    "    'abt-buy-sampled-gs_domain-complex-force',\n",
    "    'abt-buy-sampled-gs_general-simple-free',\n",
    "    'abt-buy-sampled-gs_general-complex-free',\n",
    "    'abt-buy-sampled-gs_general-simple-force',\n",
    "    'abt-buy-sampled-gs_general-complex-force',\n",
    "    \n",
    "    'abt-buy-sampled-gs_relatedwork-simple',\n",
    "    'abt-buy-sampled-gs_relatedwork-complex',\n",
    "    \n",
    "    'abt-buy-sampled-gs_domain-simple-force-rules-heterogeneity',\n",
    "    'abt-buy-sampled-gs_domain-simple-force-rules-gen-handpicked',\n",
    "    \n",
    "    'amazon-google-sampled-gs_domain-simple-free',\n",
    "    'amazon-google-sampled-gs_domain-complex-free',\n",
    "    'amazon-google-sampled-gs_domain-simple-force',\n",
    "    'amazon-google-sampled-gs_domain-complex-force',\n",
    "    'amazon-google-sampled-gs_general-simple-free',\n",
    "    'amazon-google-sampled-gs_general-complex-free',\n",
    "    'amazon-google-sampled-gs_general-simple-force',\n",
    "    'amazon-google-sampled-gs_general-complex-force',\n",
    "    \n",
    "    'amazon-google-sampled-gs_relatedwork-simple',\n",
    "    'amazon-google-sampled-gs_relatedwork-complex',\n",
    "    \n",
    "    'amazon-google-sampled-gs_domain-simple-force-rules-heterogeneity',\n",
    "    'amazon-google-sampled-gs_domain-simple-force-rules-gen-handpicked',\n",
    "    \n",
    "    'walmart-amazon-sampled-gs_domain-simple-free',\n",
    "    'walmart-amazon-sampled-gs_domain-complex-free',\n",
    "    'walmart-amazon-sampled-gs_domain-simple-force',\n",
    "    'walmart-amazon-sampled-gs_domain-complex-force',\n",
    "    'walmart-amazon-sampled-gs_general-simple-free',\n",
    "    'walmart-amazon-sampled-gs_general-complex-free',\n",
    "    'walmart-amazon-sampled-gs_general-simple-force',\n",
    "    'walmart-amazon-sampled-gs_general-complex-force',\n",
    "    \n",
    "    'walmart-amazon-sampled-gs_relatedwork-simple',\n",
    "    'walmart-amazon-sampled-gs_relatedwork-complex',\n",
    "    \n",
    "    'walmart-amazon-sampled-gs_domain-simple-force-rules-heterogeneity',\n",
    "    'walmart-amazon-sampled-gs_domain-simple-force-rules-gen-handpicked',\n",
    "    \n",
    "    'dblp-scholar-sampled-gs_domain-simple-free',\n",
    "    'dblp-scholar-sampled-gs_domain-complex-free',\n",
    "    'dblp-scholar-sampled-gs_domain-simple-force',\n",
    "    'dblp-scholar-sampled-gs_domain-complex-force',\n",
    "    'dblp-scholar-sampled-gs_general-simple-free',\n",
    "    'dblp-scholar-sampled-gs_general-complex-free',\n",
    "    'dblp-scholar-sampled-gs_general-simple-force',\n",
    "    'dblp-scholar-sampled-gs_general-complex-force',\n",
    "    \n",
    "    'dblp-scholar-sampled-gs_relatedwork-simple',\n",
    "    'dblp-scholar-sampled-gs_relatedwork-complex',\n",
    "    \n",
    "    'dblp-scholar-sampled-gs_domain-simple-force-rules-heterogeneity',\n",
    "    'dblp-scholar-sampled-gs_domain-simple-force-rules-gen-handpicked',\n",
    "    \n",
    "    'dblp-acm-sampled-gs_domain-simple-free',\n",
    "    'dblp-acm-sampled-gs_domain-complex-free',\n",
    "    'dblp-acm-sampled-gs_domain-simple-force',\n",
    "    'dblp-acm-sampled-gs_domain-complex-force',\n",
    "    'dblp-acm-sampled-gs_general-simple-free',\n",
    "    'dblp-acm-sampled-gs_general-complex-free',\n",
    "    'dblp-acm-sampled-gs_general-simple-force',\n",
    "    'dblp-acm-sampled-gs_general-complex-force',\n",
    "    \n",
    "    'dblp-acm-sampled-gs_relatedwork-simple',\n",
    "    'dblp-acm-sampled-gs_relatedwork-complex',\n",
    "    \n",
    "    'dblp-acm-sampled-gs_domain-simple-force-rules-heterogeneity',\n",
    "    'dblp-acm-sampled-gs_domain-simple-force-rules-gen-handpicked',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a7d44d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "732da7fc0ded405f908d525a6dd84491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/749 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "\nLlamaTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupstage/Llama-2-70b-instruct-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlama-2-70b-instruct-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id, use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      7\u001b[0m     model_id,\n\u001b[1;32m      8\u001b[0m     device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     rope_scaling\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdynamic\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfactor\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m2\u001b[39m} \u001b[38;5;66;03m# allows handling of longer inputs\u001b[39;00m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m transformers\u001b[38;5;241m.\u001b[39mpipeline(\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     17\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     18\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/auto/tokenization_auto.py:837\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    834\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    835\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    836\u001b[0m         )\n\u001b[0;32m--> 837\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    839\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[1;32m    840\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/utils/import_utils.py:1412\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1412\u001b[0m requires_backends(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_backends)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/utils/import_utils.py:1400\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1398\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m   1399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nLlamaTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"upstage/Llama-2-70b-instruct-v2\"\n",
    "model_name = \"Llama-2-70b-instruct-v2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_8bit=True,\n",
    "    rope_scaling={\"type\": \"dynamic\", \"factor\": 2} # allows handling of longer inputs\n",
    ")\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af23371d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attributes = ['default']\n",
    "\n",
    "for task in tasks:\n",
    "    for attribute in attributes:\n",
    "        \n",
    "        \n",
    "        # open the JSON file in read mode\n",
    "        with open(f'../tasks/{task}.json', 'r') as f:\n",
    "            # load the JSON data from the file and convert it into a dictionary\n",
    "            task_dict = json.load(f)\n",
    "        \n",
    "        # Create LangChain PromptTemplate\n",
    "\n",
    "        template = \"\"\"{task_prefix}{input_string}\"\"\"\n",
    "        prompt = PromptTemplate(\n",
    "                template=template,\n",
    "                input_variables=['task_prefix', 'input_string']\n",
    "        )\n",
    "        \n",
    "        prompts = []\n",
    "        \n",
    "        \n",
    "        if attribute == 'default':\n",
    "            for example in task_dict['examples']:\n",
    "                text_prompt = prompt.format(task_prefix=task_dict['task_prefix'], input_string=example['input'])\n",
    "                text_prompt = f\"### User: {text_prompt}\\n\\n### Assistant:\\n\"\n",
    "                prompts.append(text_prompt)\n",
    "                \n",
    "        targets = [example['target_scores'] for example in task_dict['examples']]\n",
    "        \n",
    "        streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "        \n",
    "        sequences = pipeline(\n",
    "            prompts,\n",
    "            #streamer=streamer,\n",
    "            use_cache=True,\n",
    "            max_new_tokens=100\n",
    "        )               \n",
    "\n",
    "        # Do some data wrangling to format target and preds to match squad V2\n",
    "        answers = []\n",
    "        predictions = []\n",
    "        truth = []\n",
    "        unclear_answers = 0\n",
    "        num_long_answers = 0\n",
    "        for i in range(len(targets)):\n",
    "            if targets[i]['Yes'] == 1:\n",
    "                truth.append(1)\n",
    "            else:\n",
    "                truth.append(0)\n",
    "\n",
    "            processed_pred = sequences[i][0]['generated_text'].replace(f'{prompts[i]}', '')\n",
    "            answers.append(processed_pred)            \n",
    "                    \n",
    "            # handle yes/no answers\n",
    "            \n",
    "            processed_pred = processed_pred.strip().translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "\n",
    "            if processed_pred != 'yes' and processed_pred != 'no':\n",
    "                print(f'Overlong Answer: {processed_pred}')\n",
    "                num_long_answers += 1\n",
    "            if 'yes' in processed_pred:\n",
    "                processed_pred = 'yes'\n",
    "            elif 'no' in processed_pred:\n",
    "                processed_pred = 'no'\n",
    "            else:\n",
    "                processed_pred = 'no'\n",
    "                unclear_answers += 1\n",
    "\n",
    "            if processed_pred == 'yes':\n",
    "                predictions.append(1)\n",
    "            elif processed_pred == 'no':\n",
    "                predictions.append(0)\n",
    "        \n",
    "        # save the prompts\n",
    "        with open(f'../prompts/{task}_{attribute}_{model_name}_run-1.pickle', 'wb') as handle:\n",
    "            pickle.dump(prompts, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        # save the answers\n",
    "        with open(f'../answers/{task}_{attribute}_{model_name}_run-1.pickle', 'wb') as handle:\n",
    "            pickle.dump(answers, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "        precision = precision_score(truth, predictions)\n",
    "        recall = recall_score(truth, predictions)\n",
    "        f1 = f1_score(truth, predictions)\n",
    "        accuracy = accuracy_score(truth, predictions)\n",
    "        \n",
    "\n",
    "        results = {\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1': f1\n",
    "        }\n",
    "\n",
    "        with open(f\"../results/result_{task}_{attribute}_{model_name}_run-1.json\", \"w\") as outfile:\n",
    "            json.dump(results, outfile, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
